Start with
int comm_sz
int my_rank

then:

MPI_Init(&argc, &argv);

declare total world, and individual place
int comm_sz;
int my_rank;

to give each core a rank:
MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
MPI_Comm_rank(MPI_COMM_WORLD. &my_rank);

point to point sending
MPI_Send(void message, size, MPI_DATA, dest, tag, comm_world);

MPI_Recieve(void message buffer, size, DATA, source, tag, comm, &status/MPI_STATUS_IGNORE);
  MPI_ANY_SOURCE can be used if oder isn't known

data size -> MPI_Get_count(status_p of sent, type) and MPI_Probe(status, some other stuff)


MPI_Reduce(&input, &output, count, DATAtype, MPU_operator, dest, comm)
  works with arrays, count is N, inputs are local arrays too

MPI_Allreduce distributes sum to all processes


MPI_Scatter(message, count to each, data type, recieve, recv count, recv data type, src, comm)
  Sending parts of Bulk and partitioning it off

MPI_Gather(send_message, recieves stuff in increasing order and stores each in an array)
  Allgather also exists

structs can be created too

MPI_Barrier to sync stuff

MPI Scan(sendbuf, recbuf, count, datatype, op, comm)
  returns reduction values from elements 0 to i for proc i
  prefix sum


MPI_Bcast(&data, count, DATA_type, source, comm)
  Distributes data


MPI_FLOAT






















MPI_Finalize(void);
